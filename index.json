[{"content":"In data science and engineering we tend to think in \u0026ldquo;DAGs\u0026rdquo; (directed acyclic graphs), which just means \u0026ldquo;to make this report, we first have to build this other thing, and to build that thing, we have to run these two queries, and so on. It decomposes the process of building data and artifacts like visualizations or data exports into smaller, individual chunks.\nThere are a lot of contenders in the market for selling solutions to this exact scenario, and each one solves it a little differently. Right now the hot thing is dbt, but before that we had airflow, dagster, prefect, argo, and a host of others that all were built to operate DAGs at scale on different platforms. For large, mission-critical data pipelines these can provide a lot of value, but the truth is that as data scientists, most of us don\u0026rsquo;t need something this heavy. Most projects I see really just need some way of defining the links between \u0026ldquo;scrapbook output\u0026rdquo;. Maybe it\u0026rsquo;s a jupyter notebook, or a python script, or some queries that have to happen in a particular order based on an updated warehouse feed.\nMoreover, there are some reasons you might not want to try adding an entire workflow management ecosystem into your stack. Maybe:\n You can\u0026rsquo;t get permission for a new install You don\u0026rsquo;t want to force another install on your end users or coworkers You don\u0026rsquo;t want more transitive dependencies entering the picture You don\u0026rsquo;t like someone trying to sell their cloud solution on top of the free tier offering to you  make was born from a history of compiling C programs on Unix machines in the 70\u0026rsquo;s, but it\u0026rsquo;s completely agnostic to language choice. It\u0026rsquo;s job is to translate targets and prerequisites into a DAG, and incrementally build only the parts it needs to when any of the source files change. Given that, why would I choose Make over one of the more modern alternatives?\n The commands are elegant - I enjoy the language of make report.xlsx Parallel execution is built in and easy to turn on or off It\u0026rsquo;s installed on damn near everything,1 and has proven over the last 50 years to be a shark, not a dinosaur It\u0026rsquo;s a \u0026ldquo;small\u0026rdquo; program. You can get through the documentation and start creating useful software in a couple hours. Like SQL, Make is declarative. We describe the result, and let the program optimize the route by which we get there.  Creating a simple project with make and python # I\u0026rsquo;m going to use a distilled example of a recent project I built using just a Makefile, some python, and a little SQL. By the end of this my hope is to show that simple tools can be efficient and reliable, and avoid the overhead of learning, installing, configuring, and inevitably debugging something more complex.2 Ultimately, I wanted to hand this project off in such a way that any of my teammates could maintain it if I was unavailable, so it had to be short, and stick to the tools I know they will always have installed.\nOur goal is to produce an Excel file for executive consumption that has a meaningful summary of some data pulled out of our analytics warehouse. Overall, it\u0026rsquo;ll look a little like this:\nbase queries --\u0026gt; summary CSVs --\u0026gt; (report.xlsx, diagrams for powerpoint) The \u0026ldquo;base\u0026rdquo; queries might look a lot like temporary tables or common table expressions (CTEs, or those blocks you see in WITH statements), but we\u0026rsquo;ve broken them into several, separate queries. We\u0026rsquo;re plopping those summarized results into some flat files so that we can examine the results with our favorite tools like pandas or awk. We\u0026rsquo;ll then take all those flat results and produce deliverables from them, like charts and an excel file.\nmyguy is always there for me, so that\u0026rsquo;s the name of our project, and its basic structure looks like this:\n. ├── config.yml ├── Makefile ├── myguy.py ├── README.md └── sql ├── this_quarter_sales.sql ├── model_forecast.sql └── customer_disposition.sql We will execute each file in the sql/ directory as a query we execute and then locally cache the results as CSVs. Later we\u0026rsquo;ll discuss how to handle the case where all our queries are handled remotely, and don\u0026rsquo;t create local files, such as running CREATE TABLE AS (ctas) queries prior to building the report, but for now we\u0026rsquo;ll keep it simple: query \u0026ndash;\u0026gt; csv on computer.\nOur goal is to make reproducing this report dead simple. I should only have to run this command to rebuild the report at any time:\nmake report.xlsx The cookbook that provides this rule is the Makefile.\n# Makefile report.xlsx: myguy.py python -m myguy build-report If this is your first time seeing make, there are a few terms to know:\n report.xlsx - this is the target of the rule. It is the file produced by running make report.xlsx myguy.py - the prerequisite of report.xlsx. It has to exist in order to create the excel file. If this python file\u0026rsquo;s contents have changed recently, then that\u0026rsquo;s an indication that report.xlsx may also change. python -m myguy build-report - this is the recipe that Make runs when you issue the command make report.xlsx. I am invoking python with the -m, or \u0026ldquo;run module as main\u0026rdquo; flag in case we ever refactor our single .py file into a module, like myguy/__init__.py with its complementary \u0026ldquo;dunder main\u0026rdquo; myguy/__main__.py.  Our main entry point will be in the python module. This will handle the program runtime for executing queries or doing some pandas hackery.\n# myguy.py from pathlib import Path from time import sleep import click import pandas as pd @click.group() def cli(): pass @cli.command() @click.option( \u0026#34;-o\u0026#34;, \u0026#34;--output\u0026#34;, help=\u0026#34;Write to file\u0026#34;, default=f\u0026#34;{BUILD_DIR}/report.xlsx\u0026#34;, show_default=True, ) def build_report(output: str): \u0026#34;\u0026#34;\u0026#34; Generate a new Excel workbook and save it locally. \u0026#34;\u0026#34;\u0026#34; # Suppose this produces, you know, useful data pd.DataFrame(dict(a=range(3), b=list(\u0026#34;abc\u0026#34;))).to_excel(output, index=False) sleep(2) print(f\u0026#34;Saved report to {output}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: cli() I think click is just great, and provides me with a lot of zen writing a command line interface (CLI) compared to argparse, although you could achieve everything I\u0026rsquo;m doing in this article equally well with argparse. In this script we create a cli group because we\u0026rsquo;ll eventually add more commands to it. The build_report function just replicates a process that takes a couple seconds before it outputs a file to build/report.xlsx.\nIt takes very little code to get a pleasant command line experience with nested commands. Here\u0026rsquo;s a quick example of using it after adding another command called query, which we\u0026rsquo;ll get to in a moment:\n However, if we try building our report right now with make report.xlsx, we get a FileNotFoundError: [Errno 2] No such file or directory: 'build/report.xlsx', and that\u0026rsquo;s because we need to make sure the build directory exists before running this command. We could handle that in the python with a few lines, but why not have our dependency management tool, make, do it for us?\n# Makefile build: mkdir -p build report.xlsx: myguy.py | build python -m myguy build-report Now our make report.xlsx works just fine, and we get a new directory build with our empty report in it. Normally we won\u0026rsquo;t need the |, but in this case it declares that the build rule should only be run once, even if we have other targets with build as a prerequisite.3 If we rerun make report.xlsx, it doesn\u0026rsquo;t try to create that directory again, because it already exists.\nWe do have one other problem though: if we re-run the python code, it will overwrite our report, even if nothing has changed. Instead, we should get a message saying make: 'report.xlsx' is up to date. This is happening because our target is report.xlsx instead of build/report.xlsx, so make looks in the current directory, sees that there\u0026rsquo;s no report.xlsx and therefore runs the recipe. I don\u0026rsquo;t want to write make build/report.xlsx, so what we\u0026rsquo;ll do is set up make to automatically look in our build and sql directories for files by setting the VPATH variable:\n# Makefile BUILDDIR := build SQLDIR := sql VPATH := $(SQLDIR):$(TARGETDIR) $(BUILDDIR): mkdir -p $(BUILDDIR) report.xlsx: myguy.py | $(BULDDIR) python -m myguy build-report By doing this, we can just issue make report.xlsx instead of make build/report.xlsx. Setting build to a variable (referenced via $(BUILDDIR)) allows us to change up the build directory on a whim, should we need to.\nNext, we need to structure the rules that handle our queries, so let\u0026rsquo;s add a generic method for doing exactly that, given the path to a sql file.\n# myguy.py, cont. from datetime import datetime # ...other content same as before... BUILD_DIR = \u0026#34;build\u0026#34; @cli.command() @click.argument(\u0026#34;path\u0026#34;) def query(path: str): \u0026#34;\u0026#34;\u0026#34; Issue the query located at `path` to the database, and write the results to a similarly-named CSV in the `build` directory. \\bExamples -------- This command will produce a new file foobar.csv in the `myguy.BUILD_DIR` directory: $ python -m myguy query sql/foobar.sql \u0026#34;\u0026#34;\u0026#34; # Hey look, more fake code for an article about querying data sleep(2) destination = Path(BUILD_DIR) / Path(path).with_suffix(\u0026#34;.csv\u0026#34;).name ( pd.DataFrame( dict(time_updated=[datetime.now()]) ) .to_csv(destination, index=False) ) print(f\u0026#34;Finished query for {path}and wrote results to {destination}\u0026#34;) Again, imagine that the \u0026ldquo;sleep\u0026rdquo; we\u0026rsquo;re doing here is some body of actual code that fetches results from the database. We also don\u0026rsquo;t need pandas for something as banal as touching a csv with today\u0026rsquo;s date, but it\u0026rsquo;s there to replicate the very common use case of pd.read_sql -\u0026gt; to_csv, which is nearly always the most efficient way to write a program that acquires a database connection, queries it, and writes the results to a csv for analytics-scale work in python like this.\nWe\u0026rsquo;ve also refactored out the destination build directory into a variable, so we can have the make script grab that automatically using the shell built-in:\n# Makefile BUILDDIR := $(shell python -c \u0026#39;import myguy; print(myguy.BUILD_DIR)\u0026#39;) This technique is also useful for having builds that depend on things like myguy.__version__, if it exists. With the python in place, we need to set up our recipes that run the queries. A naive first approach might look like this:\n# Makefile, cont. # ... same as above ...  this_quarter_sales.csv: this_quarter_sales.sql python -m myguy query sql/this_quarter_sales.sql model_forecast.csv: model_forecast.sql python -m myguy query sql/model_forecast.sql customer_disposition.csv: customer_disposition.sql python -m myguy query sql/customer_disposition.sql That\u0026rsquo;s a lot of repetition, so our programmer instincts should kick in here and tell us that \u0026ldquo;there must be a better way!\u0026rdquo;4. Make can handle this using implicit rules. We specify a pattern like this, and Make will do all the hard work of connecting the files together:\n%.csv: %.sql myguy.py | $(BUILDDIR) @python -m myguy query $\u0026lt; This takes care of all three rules at once.\n The % is a wildcard - it matches anything and our rules here say that \u0026ldquo;if we\u0026rsquo;re going to build a .csv, then it has a similarly-named .sql file as a prerequisite. The @ at the start of the recipe suppresses echoing of the command when Make runs it The $\u0026lt; is an automatic variable that stands for the first prerequisite. In our case, that will be the full path to the .sql file we are passing into the query function we previously wrote. Since myguy.py is the \u0026ldquo;source\u0026rdquo; of the running command, that\u0026rsquo;s also a prerequisite the | $(BUILDDIR), as before, says that we have an order-only prerequisite on the build directory, and our earlier rule will ensure mkdir build is run before trying to put output there  Now we can issue the same command as our report to build the CSV from each of our queries:\n$ make this_quarter_sales.csv Finished query for sql/this_quarter_sales.sql and wrote results to build/this_quarter_sales.csv We still need to tie all these together so that I don\u0026rsquo;t have to run each command manually - we want to just run make report.xlsx and have it do all the prerequisite queries for us. To accomplish this, we\u0026rsquo;re going to use two more built-ins, wildcard and patsubst to build the prerequisite and target lists, respectively.\nSQLFILES := $(wildcard $(SQLDIR)/*.sql) TARGETS := $(patsubst $(SQLDIR)/%.sql,%.csv,$(SQLFILES)) If we were to echo the contents of these two variables, they would look like this:\n# contents of SQLFILES sql/customer_disposition.sql sql/model_forecast.sql sql/this_quarter_sales.sql # contents of TARGETS customer_disposition.csv model_forecast.csv this_quarter_sales.csv Since our report.xlsx depends on all three of the files in TARGETS, we bind them together in that rule:\n# Makefile  # ... other content same as before ...  report.xlsx: $(TARGETS) @python -m myguy build-report Note here that we took out the myguy.py and $(BUILDDIR) prerequisites from this rule, since those are coming from our implicit rule on the $(TARGETS). I\u0026rsquo;m also going to add a clean rule for trying things over from a fresh start:\nclean: @[ ! -d $(BUILDDIR) ] || rm -r $(BUILDDIR) Decomposing this into English:\n @ - don\u0026rsquo;t echo this command when it runs. Just run it. [ ! -d $(BUILDDIR) ] || - unless the BUILDDIR is missing, do the next command5 rm -r $(BUILDDIR) - remove the contents of the BUILDDIR recursively  Here\u0026rsquo;s where our Makefile is now:\n# Makefile BUILDDIR := $(shell python -c \u0026#39;import myguy; print(myguy.BUILD_DIR)\u0026#39;) SQLDIR := sql VPATH := $(SQLDIR):$(TARGETDIR) SQLFILES := $(wildcard $(SQLDIR)/*.sql) TARGETS := $(patsubst $(SQLDIR)/%.sql,%.csv,$(SQLFILES)) report.xlsx: $(TARGETS) @python -m myguy build-report $(BUILDDIR): mkdir -p $(BUILDDIR) %.csv: %.sql myguy.py | $(BUILDDIR) @python -m myguy query $\u0026lt; clean: @[ ! -d $(BUILDDIR) ] || rm -r $(BUILDDIR) So now we can run clean and build the report:\n We can do better though. Make has parallelism built in, and most of our computers have no trouble running things concurrently. By providing the -j (jobs) flag, we can tell it to do several things at once as long as they don\u0026rsquo;t depend on one another. Since our intermediate queries to CSV fit the bill, they can all run at the same time:\n It\u0026rsquo;s also possible to enable parallelism by default with the MAKEFLAGS special variable.\n# Makefile MAKEFLAGS := --jobs=$(shell nproc) Adding dependencies between intermediate queries # Let\u0026rsquo;s suppose we add two more queries, and they need to run before our existing queries, because we did a little refactoring of our SQL.\n  Our DAG  Moreover, let\u0026rsquo;s assume that these two new tables are too large to cache locally in a flat file, and we have to issue a CREATE TABLE AS (ctas) statement to build them first. Not every database will permit you to run a CTAS on it, but imagine this is any process that writes data remotely instead of locally, such as spark writing a parquet file on S3, or submitting a POST request to an endpoint we don\u0026rsquo;t control.\nWe represent this in Make by first writing out the targets and prerequisites with different suffixes. For the remote tables, I\u0026rsquo;ll use the \u0026lsquo;.ctas\u0026rsquo; suffix. I also like to do this in a separate file, dag.mk, so I can hop to its buffer directly in my editor.\n# dag.mk sales_subset.ctas: customer_product.ctas: this_quarter_sales.csv model_forecast.csv: sales_subset.ctas customer_disposition.csv: customer_product.ctas This arrangement forces the completion of sales_subset.csv and customer_product.csv prior to the original three queries. Then in the main MakeFile, we include these contents above the rules that handle .csv files, along with a new rule for handling the remote tables. The %.ctas rule will create an empty target as soon as the query is done, signaling to make when it last completed successfully:\n# Makefile # ... other content the same ... include dag.mk %.csv: %.sql myguy.py | $(BUILDDIR) @python -m myguy query $\u0026lt; %.ctas: %.sql myguy.py | $(BUILDDIR) @python -m myguy ctas $\u0026lt; @touch $(BUILDDIR)/$@ Note that this includes a new command for myguy, so let\u0026rsquo;s add that too:\n# myguy.py @cli.command() @click.argument(\u0026#34;path\u0026#34;) def ctas(path: str): \u0026#34;\u0026#34;\u0026#34; Perform a CREATE TABLE AS statement from the SELECT statement in the given SQL file path. \u0026#34;\u0026#34;\u0026#34; sql_file = Path(path) query = f\u0026#34;CREATE TABLE {sql_file.stem}AS {sql_file.read_text()}\u0026#34; print(query) sleep(3) # imagine the query is running *** TODO could also do it via adding another directory under target for the csv queries***\nAnd one last thing - our $(TARGETS) assignment has no way of telling which sql files it should or shouldn\u0026rsquo;t tie to CSVs. The easiest way to make this distinction is to actually just remove $(TARGETS) altogether, and have the dag.mk declare what report.xlsx depends on.\n# dag.mk # ...other contents the same... report.xlsx: this_quarter_sales.csv \\ model_forecast.csv \\ customer_disposition.csv # Makefile  report.xlsx: @python -m myguy build-report Make will take care of combining these two rules into a single one. If all the targets are going to have the same suffix, such as .ctas or .csv, then the trick with $(TARGETS) is handy, but adds more complexity than it\u0026rsquo;s worth when mixing target file types.\nAltogether, our dag now looks like this:\n Templating the SQL with Jinja # To round out some of our feature parity with dbt, we need to add templating to our SQL. It\u0026rsquo;s best if I don\u0026rsquo;t have to think about how the template is injected, I just want a standard place to put stuff and have the python module take care of it. Let\u0026rsquo;s introduce a configuration file:\n# config.yml sales_max_date: \u0026#34;2022-03-01\u0026#34; sales_min_date: \u0026#34;2021-09-01\u0026#34; Presumably, a future analyst will have to come refresh this report for a different set of dates, and we want that configuration readily available to them. The templating engine we\u0026rsquo;ll use is the same on available on dbt, Jinja2. How we integrate it is by intercepting our python code that reads queries and applying the template from the config there. The jinja2 part is a little verbose, but flexible - anything that\u0026rsquo;s in our config will be available to the templated SQL:6\n# myguy.py import yaml import jinja2 # ... other content the same ... BUILD_DIR = \u0026#34;build\u0026#34; PROJECT_DIR = Path(__file__).parent def get_config(): with open(PROJECT_DIR / \u0026#34;config.yml\u0026#34;) as f: config = yaml.safe_load(f) return config # The `str | Path` union type hint is python 3.10 syntax, so watch out if you\u0026#39;re # on an older version! def read_sql_text(path: str | Path) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Read a SQL file contents and apply jinja templating from this project\u0026#39;s config.yml \u0026#34;\u0026#34;\u0026#34; config = get_config() jinja_loader = jinja2.FileSystemLoader(PROJECT_DIR) jinja_environment = jinja2.Environment(loader=jinja_loader) template = jinja_environment.get_template(str(path)) sql = template.render(**config) return sql And now we use this version of the sql in the ctas and query functions:\n# myguy.py # ...other content the same ... def query(path: str): sql = read_sql_text(path) destination = Path(BUILD_DIR) / Path(path).with_suffix(\u0026#34;.csv\u0026#34;).name # Assuming you have some way of acquiring a database connection with get_connection() as conn: pd.read_sql(sql, conn).to_csv(destination) def ctas(path: str): path = Path(path) sql = f\u0026#34;CREATE TABLE `{path.stem}` AS {read_sql_text(path)}\u0026#34; with get_connection() as conn: conn.execute(sql) Within the SQL itself, we can now reference any of the keys in the config.yml directly:\n#sales_subset.sqlSELECTproduct_id,sales_revenue,sales_unitsFROMfact_salesWHEREsales_dateBETWEENdate(\u0026#39;{{ sales_min_date }}\u0026#39;)ANDdate(\u0026#39;{{ sales_max_date }}\u0026#39;)I\u0026rsquo;m not here to cover everything you can do with jinja and yaml, since those are already pretty well covered. If you haven\u0026rsquo;t used it before, it\u0026rsquo;s worth looking into. It\u0026rsquo;s very powerful. The looping and conditional constructs can make what would normally be pretty tough with just raw SQL easy. When we issue a make recent_sales.ctas command, it looks like this:\n$ make sales_subset.ctas mkdir -p build CREATE TABLE `sales_subset` AS SELECT product_id , sales_revenue , sales_units FROM fact_sales WHERE sales_date BETWEEN date(\u0026#39;2021-09-01\u0026#39;) AND date(\u0026#39;2022-03-01\u0026#39;) \nConclusion # There we have it, a simple little dag system for coordinating our project\u0026rsquo;s deliverables. For a working example of how to implement this project structure, check out the companion repo, which builds a simple analysis on the chinook dataset. This example also includes code that produces .png files for including into presentations and migrates the mypy.py into a fully-fledged, pip-installable module.\n  Except Windows. You\u0026rsquo;ll need to get it via mingw/cygwin or via the Windows subsystem for Linux.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I fully acknowledge the irony here that make is, in fact, a very foreign tool to many data scientists.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n These are called order-only prerequisites\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I\u0026rsquo;m taking this phrase from Raymond Hettinger, who gives fantastic talks on writing idomatic python. I recommend his beyond PEP8 talk to all levels of developers.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n We do \u0026ldquo;unless\u0026rdquo; instead of the more familiar \u0026ldquo;if\u0026rdquo; statement, because if we did this: [ -d $(BUILDDIR) ] \u0026amp;\u0026amp; rm -r $(BUILDDIR) the test command [ exits with status 1 when the build directory doesn\u0026rsquo;t exist, and hence the whole pipe exits status 1. Make treats that as a failed recipe, which isn\u0026rsquo;t what we intend. We want it to look like a success both in the case of removing the directory should it exist, and doing nothing if it doesn\u0026rsquo;t.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n You should be extremely careful about templating like this. This article also uses f-strings for injecting arbitrary code into SQL, which is unacceptable if any of your system is public facing. All of these examples are working under the assumption that we\u0026rsquo;re in a locked-down internal data warehouse, and someone who has access to the system in any way is allowed to issue an arbitrary query.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"6 March 2022","permalink":"/posts/000_make_dag/","section":"Posts","summary":"In data science and engineering we tend to think in \u0026ldquo;DAGs\u0026rdquo; (directed acyclic graphs), which just means \u0026ldquo;to make this report, we first have to build this other thing, and to build that thing, we have to run these two queries, and so on.","title":"Building DAGs with Make, SQL, and python"},{"content":"Why create a new tech blog when so many great ones already exist? In short, because data scientists often get the advice that they should \u0026ldquo;improve their software engineering skills,\u0026rdquo; but the advice they are given on how to do so is usually terrible1. Data science, from a programming perspective, is in a weird place. We aren\u0026rsquo;t quite front end, but we create maps, charts, and plots to display in the browser all the time. We aren\u0026rsquo;t quite back end, yet we write code that targets the file system, databases, and external APIs. And we aren\u0026rsquo;t quite consultants, but still have to be the expert, data-driven decision maker. Data science is not a slow moving field, either, which leaves little space for pausing to reflect on how to write better code.\nI primarily work in a world that uses python, SQL, the standard GNU toolkit (bash, make, vim), and the usual host of data-sciency things that come with all that, like Jupyter and plotly. My posts will be largely targeted at an audience who intends to become more efficient with these tools, or to introduce those who want to become better coders, but aren\u0026rsquo;t sure where to start, to some of the tools that can improve your data science workflow. For about five years now, I\u0026rsquo;ve mentored Data Science team members from writing their first line of code to maintaining 100k \u0026ldquo;source lines of code\u0026rdquo; (SLOC) code bases2. I\u0026rsquo;ve received (and given) many questions prefaced with \u0026ldquo;this may be a stupid question, but\u0026hellip;\u0026rdquo;, and thought that it was about time I wrote down the answers for others to learn from.\n  As of writing, it\u0026rsquo;s February of 2022 and I still see Reddit comments telling new python programmers that requirements.txt is an acceptable form of project dependency management.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Seasoned programming veterans out there might scoff at how low 100k sounds, but for those accustomed to working mainly in Jupyter notebooks, this can be a daunting amount of code.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"26 February 2022","permalink":"/posts/my-first-post/","section":"Posts","summary":"Why create a new tech blog when so many great ones already exist?","title":"Objective"}]