[{"content":" I\u0026#39;m pretty happy with my (neo)vim setup. Wielding it, I can walk into most code bases and be useful. There comes a point, though, where we hit a wall, look at Jupyter or VSCode, and think \u0026#34;damn, that actually is pretty useful.\u0026#34; This started happening more frequently as my workflow transitioned to terminal-based tooling for Python and away from Jupyter and PyCharm. Suddenly, I found myself lacking some of the basics we need to do useful data science, like rich text (math) and images (plots). I don\u0026#39;t want to give up my precious all-about-me editing experience, though, and if I can avoid proprietary software, I will.\nLike most fledgling data scientists, my journey began on a Jupyter notebook, followed by JupyterLab, when it came out. After learning that \u0026#34;hey, I can make my own libraries too!\u0026#34;, I found Spyder from the Anaconda ecosystem, jumped to PyCharm for a while, all the while using a dash of Vim on the side. Recently, I went all-in on a lua configuration for Neovim, and that\u0026#39;s been my home for the last couple years now, except for when I just need a notebook. Typically I still turn to JupyterLab when I need a literate programming environment, with access to plotting inline with code. Sometimes a notebook is just the right thing for the job. Other times, I need a text editor and a language server for some back-end work. I want a free/libre environment that supports both of these, and right now I don\u0026#39;t think the options are great.\nSpyder is probably the closest right now, but I have the least experience with it. The messaging from their website is wholly python-centric, though, and I\u0026#39;m looking for a much more generic tool. I also have no idea how well it handles remote project development VSCode is currently the gold standard in terms of \u0026#34;just working\u0026#34; out of the box. I haven\u0026#39;t seen anything integrate with remote development, including Jupyter notebooks, as well as this one does. It doesn\u0026#39;t meet the \u0026#34;libre\u0026#34; requirement, though, and Microsoft has no plans for opening its pylance server either PyCharm is a behemoth powerhouse for working on a local codebase. It absolutely falls apart when trying to work on remote projects Vim and Neovim are terminal editors, and by their very nature are not designed for displaying images. Really, they are designed to address the physical act of editing text, and leave the other components of your development workflow to be integrated some other way (i.e. running commands in one tmux window and vim in the other) I think Pluto is incredibly slick, and I wish more notebooking environments operated like it So where does this leave us? Emacs has:\nRich text and image display (as a GUI program) Inline literate programming via org-mode, and many Jupyter integration projects Support for basically every programming language on the planet Tree-sitter and Language Server Protocol (LSP) support, with the option to choose our own server A fully featured general programming language for configuration and extension (Emacs LISP) An enormous integrated help and documentation system Cross-platform to Windows, Linux, and macOS Libre license (GNU GPL 3.0) Built-in remote development support (via TRAMP) Terminal emulation via any of `M-x term`, `M-x shell`, or `M-x eshell` All that to say Emacs is a tempting offer, and I\u0026#39;m going to try diving in so that you don\u0026#39;t have to. The plan is to build an emacs configuration from scratch with the goal of a data science workstation in mind, and will be aimed at folks like me - people who want to learn how this crazy emacs monster works, and are maybe a strong Vimmer, but haven\u0026#39;t had the chance to really sit down an learn emacs yet. As such, this will be more than a \u0026#34;follow-along\u0026#34; configuration guide; rather I\u0026#39;m aiming to dig into the details, and weigh the merits of choosing one thing over another, especially as they compare against their (neo)vim counterparts. As long as I can keep up, the plan is one post a week, focusing on a single component to integrate, such as LSP, auto-complete, remote workflow, notebooks, packaging, Windows-specific forays, and so on. I may reference back to this article and change it up a bit as we learn more, so that this article can be the one-stop-shop on justification for \u0026#34;why emacs\u0026#34;?\nAt the end of the day, though, this whole process is largely to document my own learning process, so I can come back and say \u0026#34;why on earth did I do it this way? Oh, that\u0026#39;s rightâ€¦\u0026#34; There are already an enormous number of excellent learning materials out there for picking up emacs, so my recommendation for other people like me is to also give them a shot:\nThe official Emacs manual An Introduction to Programming in Emacs Lisp Mastering Emacs ","date":"31 July 2022","permalink":"/posts/004_emacs_start/","section":"Posts","summary":"I\u0026#39;m pretty happy with my (neo)vim setup.","title":"Emacs Has Been Waiting for Data Science"},{"content":"Some time ago, I saw this fantastic talk by Sandy Metz on favoring object composition over inheritance.\nAt the 17:00 mark, she introduces the cumulative folk tale This Is the House That Jack Built, the first few lines of which looks like this:\nThis is the house that Jack built. This is the rat that ate the cheese that lay in the house that Jack built. This is the dog that worried the cat that chased the rat that ate the cheese that lay in the house that Jack built. This is the cow with the crumpled horn that tossed the dog that worried the cat that chased the rat that ate the cheese that lay in the house that Jack built. This is the maiden all forlorn that milked the cow with the crumpled horn that tossed the dog that worried the cat that chased the rat that ate the cheese that lay in the house that Jack built. \u0026hellip;\nFor the rest of the talk, she focuses on the problem of programming a class that can perform two transformations on the poem:\nRandomize the order in which lines are added, so that the \u0026ldquo;rat that ate the cheese\u0026rdquo; might come before the \u0026ldquo;maiden all forlorn\u0026rdquo; \u0026ldquo;Echo\u0026rdquo; each line, so that we get \u0026ldquo;the cow with the crumpled horn that tossed the cow with the crumpled horn\u0026rdquo; That is, a user should be able to instantiate a class that can recite the poem verbatim, recite a randomized poem, recite an echoed poem, or recite a poem that\u0026rsquo;s both random and echoed. What makes this interesting as a data scientist or engineer is that she\u0026rsquo;s tackling the problem of object composition in the context of a data pipeline. So in this article, I\u0026rsquo;m going to cover Python solutions to this problem in three broad strokes:\nWhat does a literal translation of the object-oriented version look like in Python, while still remaining \u0026ldquo;pythonic?\u0026rdquo; How can we extend the code to swap the order in which transformations happen? How can we simplify the user experience by translating the logic to pure functions? On point 3 - I tend to believe a functional style, where data is immutable and pure functions create new, transformed data is usually the right approach to any system that\u0026rsquo;s \u0026ldquo;data first\u0026rdquo;. It also reduces overhead for most users by avoiding the introduction of a new object type. That\u0026rsquo;s not always a good thing, but specifically in the context of a Python end user, this means they need only remember the recite() function, and not both the House() object and its recite() method. While not strictly a functional language, Python does offer some key functional components, namely first-class functions and currying, so we\u0026rsquo;ll take a look at how those can still be useful even when building a more object-centric solution.\nThe Object-Oriented Python Solution to RandomEchoHouse # This article uses Python 3.10 syntax. To run examples on older versions of Python, some adjustments to the type annotations are required. First, let\u0026rsquo;s set up a new Python file random_echo.py with some imports we\u0026rsquo;ll need, the poem\u0026rsquo;s data as a module constant1, and a couple type aliases to make future code more readable:\n#!/usr/bin/env python3 import random from typing import Callable # requires pip install of `more-itertools` from more_itertools import always_iterable Poem = list[str] PoemTransform = Callable[[Poem], Poem] HOUSE_POEM = [ \u0026#34;the horse and the hound and the horn that belonged to\u0026#34;, \u0026#34;the farmer sowing his corn that kept\u0026#34;, \u0026#34;the rooster that crowed in the morn that woke\u0026#34;, \u0026#34;the judge all shaven and shorn that married\u0026#34;, \u0026#34;the man all tattered and torn that kissed\u0026#34;, \u0026#34;the maiden all forlorn that milked\u0026#34;, \u0026#34;the cow with the crumpled horn that tossed\u0026#34;, \u0026#34;the dog that worried the cat that chased\u0026#34;, \u0026#34;the rat that ate the cheese that lay in\u0026#34;, \u0026#34;the house that Jack built\u0026#34;, ] So from now on, a Poem is any list of string values, just like HOUSE_POEM, and a PoemTransform is any function that takes in a Poem as its only argument and returns a Poem.\nOur objective is to produce variations on this poem using a single interface:\nRecite the original poem Recite a version of the poem in random order Recite a version of the poem with each line \u0026ldquo;echoed\u0026rdquo; (duplicated) Recite the poem both in random order and with duplicated lines There are three possible transformations of a poem - we echo it, we randomize it, or we do nothing. The fourth option is a composition of the two other non-identity transformations, so we don\u0026rsquo;t consider it a separate object. Ruby has a much stricter object-oriented paradigm than Python, so Sandy\u0026rsquo;s example uses a dedicated class with a single method for each role. Such ceremony isn\u0026rsquo;t required in Python, though. We can just define a pure function for each processing step.\n# --snip-- def identity(x: Any) -\u0026gt; Any: return x def random_order(poem: Poem, random_seed: int = 42) -\u0026gt; Poem: random.seed(random_seed) return random.sample(poem, len(poem)) def echo_format(poem: Poem) -\u0026gt; Poem: return [f\u0026#34;{line} {line}\u0026#34; for line in poem] To start, let\u0026rsquo;s look at a literal translation of Sandy\u0026rsquo;s House class into Python:\n# --snip-- class House: def __init__( self, order: PoemTransform = identity, fmt: PoemTransform = identity, ): self.lines = order(fmt(HOUSE_POEM)) def recite(self, stanza: int | Sequence[int] | None = None) -\u0026gt; None: if stanza is None: indices = range(len(self.lines)) else: indices = always_iterable(stanza) for i in indices: stanza_lines = self.lines[-(i + 1) :] joined = \u0026#34;\\n\u0026#34;.join(stanza_lines) print(\u0026#34;This is \u0026#34;, joined, \u0026#34;.\u0026#34;, sep=\u0026#34;\u0026#34;, end=\u0026#34;\\n\\n\u0026#34;) Anyone who\u0026rsquo;s seen Jack Diederich\u0026rsquo;s Stop Writing Classes should notice a red flag here. We have two methods, one of which is __init__(), so that means this class is really just an obfuscated call to a recite function. In the next section we\u0026rsquo;ll refactor this down to a flatter API, but for the moment let\u0026rsquo;s just examine how this class works by dropping into an interactive session:\n$ python3 -i random_echo.py \u0026gt;\u0026gt;\u0026gt; house = House() \u0026gt;\u0026gt;\u0026gt; house.recite() # the whole tale This is the house that Jack built. ... the rat that ate the cheese that lay in the house that Jack built. \u0026gt;\u0026gt;\u0026gt; house.recite(2) # just stanza 2 This is the dog that worried the cat that chased the rat that ate the cheese that lay in the house that Jack built. \u0026gt;\u0026gt;\u0026gt; # We can \u0026#34;plug in\u0026#34; any function for the `order` role \u0026gt;\u0026gt;\u0026gt; random_house = House(order=random_order) \u0026gt;\u0026gt;\u0026gt; random_house.recite(4) This is the maiden all forlorn that milked the rat that ate the cheese that lay in the rooster that crowed in the morn that woke the judge all shaven and shorn that married the dog that worried the cat that chased. \u0026gt;\u0026gt;\u0026gt; # Similarly for the `fmt` role \u0026gt;\u0026gt;\u0026gt; echo_house = House(fmt=echo_format) \u0026gt;\u0026gt;\u0026gt; echo_house.recite(3) This is the cow with the crumpled horn that tossed the cow with the crumpled horn that tossed the dog that worried the cat that chased the dog that worried the cat that chased the rat that ate the cheese that lay in the rat that ate the cheese that lay in the house that Jack built the house that Jack built. \u0026gt;\u0026gt;\u0026gt; # Including both at once \u0026gt;\u0026gt;\u0026gt; random_echo_house = House(order=random_order, fmt=echo_format) \u0026gt;\u0026gt;\u0026gt; random_echo_house.recite() This is the dog that ... Feature Request: Line Numbers # \u0026ldquo;Can we get line numbers before each of the chunks even when randomizing? It makes it easier to read.\u0026rdquo;\nEasy. We just make a new formatter:\ndef linum_format(poem: Poem) -\u0026gt; Poem: return [f\u0026#34;{i}: {line}\u0026#34; for i, line in enumerate(poem)] random_line_house = House(fmt=linum_format, order=random_order) random_line_house.recite(9) # This is 0: the farmer sowing his corn that kept # 1: the horse and the hound and the horn that belonged to # 2: the man all tattered and torn that kissed # ... \u0026ldquo;We noticed something,\u0026rdquo; our client says. \u0026ldquo;It looks like the randomization happens before the line numbers are made. What we really wanted was to keep the original line numbers, so we know what happened. But that\u0026rsquo;s fine, we were able to just swap the two functions and now it\u0026rsquo;s working great!\u0026rdquo;\nTo our horror, we open their code and see this:\nmyhouse = House(fmt=random_order, order=linum_format) myhouse.recite(9) # This is 1: the farmer sowing his corn that kept # 0: the horse and the hound and the horn that belonged to # 4: the man all tattered and torn that kissed # ... And even worse:\ndef mynumbers(p): return linum_format(echo_format(x)) myhouse2 = House(order=mynumbers, fmt=random_order) Uh oh. We baked the ordering into House.__init__, and because we didn\u0026rsquo;t provide a generic enough API for composing functions, it\u0026rsquo;s getting used in a way we didn\u0026rsquo;t expect, which will certainly put mental burden on future maintainers as well. We now have three options:\nForce an API change that prevents the situation above Deprecate the House class and point users to a newer, better function Open up the public interface with a little more flexibility, at the expense of directly representing business logic In my experience, #1 is rarely prudent. # 2 may or may not be appropriate, depending on what the actual product is. However, as library authors it\u0026rsquo;s our responsibility to keep the public interface as consistent as possible over time. So let\u0026rsquo;s explore what it means to abstract our code a little to achieve better \u0026ldquo;pluggability\u0026rdquo;:\nFunctions First # I want to take it all the way back to the drawing board. What\u0026rsquo;s the simplest part we can keep the same? Probably all of linum_format, echo_format, and random_order remain unchanged.\nFollowing that, we need a small adjustment to the recite function: given a Poem, just print it out on the correct stanza.\n#!/usr/bin/env python3 # Identical to the `House` class version, but doesn\u0026#39;t # rely on stateful `self.lines` def recite(poem: Poem, stanza: int| Sequence[int] | None = None) -\u0026gt; None: if stanza is None: indices = range(len(poem)) else: indices = always_iterable(stanza) for i in indices: stanza_lines = poem[-(i + 1) :] joined = \u0026#34;\\n\u0026#34;.join(stanza_lines) print(\u0026#34;This is \u0026#34;, joined, \u0026#34;.\u0026#34;, sep=\u0026#34;\u0026#34;, end=\u0026#34;\\n\\n\u0026#34;) With that totally compartmentalized, now we can focus entirely on the composition part.\nNotice that the House class utilized one stateful object - the transformed poem after applying the order and fmt functions. This got stored in the self.lines attribute, and subsequent calls for specific stanzas didn\u0026rsquo;t have to re-transform the poem. _recite_stanza just read the data and printed it. Depending on how expensive we expect the functions to be, we can either keep this behavior, or switch to a version where we transform the poem each time we pass it in to recite. All in all, though, it\u0026rsquo;s impossible for us as library authors to predict which of these cases our users will be bound to, so it\u0026rsquo;s actually a poor design in the first place to force this data to persist in memory without their consent. The recite function now takes any poem. So we can pass in a transient, quickly garbage-collected one like this:\nrecite(echo_format(HOUSE_POEM)) Or collect a transformed version, persist it, and pass that in:\nrandom_echo_house = echo_format(random_order(HOUSE_POEM)) recite(random_echo_house) In the end, only the developers implementing the data (HOUSE_POEM) and the functions that act on it (random_order and echo_format) will know which of the two approaches above is appropriate, so we should give them that freedom.\nNext, we have the problem of arbitrary function composition. It\u0026rsquo;s a bit clunky to manually produce each composition like this:\ndef random_echo(poem: Poem) -\u0026gt; Poem: return echo_format(random_order(poem)) def echo_linum(poem: Poem) -\u0026gt; Poem: return linum_format(echo_format(poem)) # ... likewise for other combinations Can we provide a generic factory that lets users define a new transformation pipeline on the fly? How about a function that takes a variable set of functions as arguments, and returns a PoemTransform?\ndef compose(*funcs: PoemTransform) -\u0026gt; PoemTransform: def pipeline(poem: Poem) -\u0026gt; Poem: for f in funcs: poem = f(poem) return poem return pipeline linum_echo_random = compose(linum_format, echo_format, random_order) recite(linum_echo_random(HOUSE_POEM), stanza=9) # This is 1: the farmer sowing his corn that kept 1: the farmer sowing his corn that kept # 0: the horse and the hound and the horn that belonged to 0: the horse and the hound and the horn that belonged to # 4: the man all tattered and torn that kissed 4: the man all tattered and torn that kissed # ... It might not look like much, but because most programmers prefer reading function application from left-to-right rather than inside-out (mathematicians being the notable holdout here), some may prefer this. If our functions had varying input and output types, I would keep the slightly clunkier version where we explicitly compose functions via def and return f1(f2(...)) solely for the reason of having the pyright static type checker ensure that I\u0026rsquo;ve chained inputs and outputs correctly. Since all of our functions are PoemTransform, though, we don\u0026rsquo;t need to worry about type checking within the compose function. That is, all the input and output types are Poem, so the resulting function chain is safe.\nA note on mixins # At around the 23:35 mark, an audience member shouts out \u0026ldquo;use multiple inheritance!\u0026rdquo; to which Sandy says \u0026ldquo;just stop that - we\u0026rsquo;re not using multiple inheritance here, it\u0026rsquo;s not the right solution for this problem.\u0026rdquo;\nIt\u0026rsquo;s interesting to note that scikit-learn does exactly this as a way of composing model behavior. Every ridge regression is a regression, after all, and it will always need the attributes that come with a regression, such as its coefficient of determination and fit() method.\nPython doesn\u0026rsquo;t actually have \u0026ldquo;constants\u0026rdquo;, but by convention an all-uppercase variable is meant to signify it\u0026rsquo;s supposed to be constant\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"4 May 2022","permalink":"/posts/003_random_echo_house/","section":"Posts","summary":"Some time ago, I saw this fantastic talk by Sandy Metz on favoring object composition over inheritance.","title":"Data Pipelines as Function Composition"},{"content":"It took me a while to admit it, but with a little upfront time investment, the command line is probably the fastest way to get most tasks accomplished. For me, most tasks means short, one-off, common file operations, such as:\nTrying to find that one damn file that\u0026rsquo;s nested somewhere down all those folders Figuring out what version(s) of python I have installed, and making new virtual environments Downloading files from a link and unzipping the results Examining and editing the contents of a file Can an IDE or VScode offer all this to me? Yup. Those tools are awesome, but when I\u0026rsquo;m on the phone with someone, and they say \u0026ldquo;hey, can you pull up that one thing real fast?\u0026rdquo; I don\u0026rsquo;t have time to boot JetBrains, nor do I want to dig through VScode menus. I pop open the terminal, fuzzy search where I need to go, and hit the file with vim if it\u0026rsquo;s text or open1 if it\u0026rsquo;s something like Excel or PowerPoint. The process of getting that file open from a cold start is around 10 seconds. Here\u0026rsquo;s an example where I\u0026rsquo;m looking for a copy of the Python logo:\nfzf here indexed over a million files on my computer, but because I was able to find it using just a few key words I knew were in the file path or name.\nLet\u0026rsquo;s say it takes on average around 30 seconds to find a file clicking through a file manager.2 That\u0026rsquo;s 20 seconds of savings per file. Suppose we only look for around a dozen files like this per work day. Back of napkin math tells us:\n20 seconds/file * 12 files/day * 5 days/wk / 3600 s/hr ------------- 0.33 hours/wk So assuming you work with roughly the numbers above, one hour of getting comfortable with fzf will pay for itself in under three weeks. Scale this against the number of files you open, and how deeply nested down a mounted SharePoint folder they might be, and the dividends are much faster. If your work looks anything like mine, you\u0026rsquo;re sifting through at least several dozen spreadsheets, presentations, and source code files every day, many of them with similar names but with v3.pptx or v_FINAL.xlsx tacked on the end.\nDoes everyone need to use a fuzzy-finder to find and open files? Certainly not. Some Unix die-hards abhor the use of fuzzy-finders in their workflow, but I just can\u0026rsquo;t seem to get a pure \u0026ldquo;unixy\u0026rdquo; way to work nearly as fast as ctrl+t followed by slapping the keyboard with letters that might be somewhere in that file name.3 I also don\u0026rsquo;t think comments in the spirit of the linked /u/romainl comment have the same set of assumptions about what a \u0026ldquo;typical\u0026rdquo; data science setup looks like. I haven\u0026rsquo;t worked professionally as a website developer, but I have a feeling we work in very different environments. Often I\u0026rsquo;m sitting in front of a data warehouse I\u0026rsquo;ve never connected to before, with 2,000 unique table names, each with possibly 200+ columns. Usually the first thing I do is write a small fzf window that lets me search columns or table names. \u0026ldquo;Are there any features related to customer age? Did an excel sheet from last month make it into the data lake?\u0026rdquo; Interactive, visual feedback as I type these things, followed by a ctrl+u to clear the search bar is way faster than building a pipe with find and/or grep and examining the results each time.\nResources # fzf installation instructions kitty terminal \u0026ndash; This is how I can icat an image in the terminal This is a macOS command. On Windows, just type the name of the file. For Linux I usually am on Gnome desktop, which uses gio open\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;d love some actual hard numbers here, but I\u0026rsquo;d consider this a conservative estimate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThat said, find . -name \u0026quot;whatever.csv\u0026quot; is definitely still useful in a lot of cases\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"18 April 2022","permalink":"/posts/002_in_defense_of_fuzzy_finders/","section":"Posts","summary":"It took me a while to admit it, but with a little upfront time investment, the command line is probably the fastest way to get most tasks accomplished.","title":"In Defense of Fuzzy Finders"},{"content":"If you aren\u0026rsquo;t regularly wiping and rebuilding your virtual environments, you should be; anyone trying to run your project for the first time will thank you for it. For most folks that\u0026rsquo;s using python -m venv .venv to create a new one, or conda create if you\u0026rsquo;re on the conda stack. I do this so frequently that it became my first bash alias. In sh, bash, and most other shells, there\u0026rsquo;s more than one way to make longer commands shorter, the easiest of which is the alias:\nalias so=\u0026#34;source .venv/bin/activate\u0026#34; After running this, when we use so as the first command, it will be replaced with the text source .venv/bin/activate. Remembering to just type so once I cd to a project is much easier to remember and type quickly. If we don\u0026rsquo;t already have an environment, we usually have to create it:\nalias new-venv=\u0026#34;python -m venv .venv\u0026#34; To chain two operations together in bash, we use \u0026amp;\u0026amp; to allow the second part to run only if the first one succeeded:\nnew-venv \u0026amp;\u0026amp; so Beyond that, we usually have to upgrade pip in the new environment\nnew-venv \u0026amp;\u0026amp; so \u0026amp;\u0026amp; pip install --upgrade pip wheel And this chain is so common, I actually have the entire thing under the new-venv name, but as a bash function so it can take arguments:\nnew-venv() { local name=${1:-.venv} local python_version=${2:-3} python${python_version} -m venv $name \\ \u0026amp;\u0026amp; source $name/bin/activate \\ \u0026amp;\u0026amp; python3 -m pip install --upgrade pip wheel } The usage of this function is like this:\nnew-venv [NAME] [PYTHON_VERSION] with both arguments optional. The voodoo magic on the first two lines inside the function just says:\nAssign the value of the first argument to the name variable, and set it to \u0026ldquo;.venv\u0026rdquo; if nothing is passed in Assign the value of the second parameter to the python_version variable, and set it to \u0026ldquo;3\u0026rdquo; if nothing is passed in Physical savings might only be a few letters, but there\u0026rsquo;s a real cognitive benefit to building out your most common operations as aliases or functions. You can think at higher levels of operation, with four to five commands clicked together instead of just the current one. Often I go \u0026ldquo;I call uncle! Let\u0026rsquo;s try with a fresh environment\u0026rdquo;\ndeactivate \u0026amp;\u0026amp; rm -rf .venv \u0026amp;\u0026amp; new-venv \u0026amp;\u0026amp; poetry install Raymond Hettinger says we have a buffer in our mind of \u0026ldquo;about five things, plus or minus two\u0026rdquo;. By reducing the process above to only four steps, even on a bad day I can remember how to do this.\nTo make sure these aliases/functions are available every time you log in, add them to your \u0026ldquo;rc\u0026rdquo; file. For most folks that\u0026rsquo;s ~/.bashrc, but fish users would use funced and funcsave, zsh users have ~/.zshrc, and on Windows there\u0026rsquo;s a host of options (I wouldn\u0026rsquo;t try all this in cmd).\nI actually like the funced idea from fish a lot, so I use something similar. This allows me to edit any function in my ~/.bash_functions folder, which is then loaded up using load-funcs (also a function), and that is what gets executed by my ~/.bashrc. This gives me the chance to very quickly save useful snippets like what\u0026rsquo;s above for later. In particular, instead of looking up the right invocation to install poetry every time, I just tucked it away into the install-poetry function when I first ran it, and now it\u0026rsquo;s ready for me everywhere I take my dotfiles.\n","date":"6 April 2022","permalink":"/posts/001_faster_pyvenv/","section":"Posts","summary":"If you aren\u0026rsquo;t regularly wiping and rebuilding your virtual environments, you should be; anyone trying to run your project for the first time will thank you for it.","title":"Manage Python Environments Faster With Aliases and Functions"},{"content":"In data science and engineering we tend to think in \u0026ldquo;DAGs\u0026rdquo; (directed acyclic graphs), which just means \u0026ldquo;to make this report, we first have to build this other thing, and to build that thing, we have to run these two queries, and so on. It decomposes the process of building data and artifacts like visualizations or data exports into smaller, individual chunks.\nThere are a lot of contenders in the market for selling solutions to this exact scenario, and each one solves it a little differently. Right now the hot thing is dbt, but before that we had airflow, dagster, prefect, argo, and a host of others that all were built to operate DAGs at scale on different platforms. For large, mission-critical data pipelines these can provide a lot of value, but the truth is that as data scientists, most of us don\u0026rsquo;t need something this heavy. Most projects I see really just need some way of defining the links between \u0026ldquo;scrapbook output\u0026rdquo;. Maybe it\u0026rsquo;s a jupyter notebook, or a python script, or some queries that have to happen in a particular order based on an updated warehouse feed.\nMoreover, there are some reasons you might not want to try adding an entire workflow management ecosystem into your stack. Maybe:\nYou can\u0026rsquo;t get permission for a new install You don\u0026rsquo;t want to force another install on your end users or coworkers You don\u0026rsquo;t want more transitive dependencies entering the picture You don\u0026rsquo;t like someone trying to sell their cloud solution on top of the free tier offering to you make was born from a history of compiling C programs on Unix machines in the 70\u0026rsquo;s, but it\u0026rsquo;s completely agnostic to language choice. It\u0026rsquo;s job is to translate targets and prerequisites into a DAG, and incrementally build only the parts it needs to when any of the source files change. Given that, why would I choose Make over one of the more modern alternatives?\nThe commands are elegant - I enjoy the language of make report.xlsx Parallel execution is built in and easy to turn on or off It\u0026rsquo;s installed on damn near everything,1 and has proven over the last 50 years to be a shark, not a dinosaur It\u0026rsquo;s a \u0026ldquo;small\u0026rdquo; program. You can get through the documentation and start creating useful software in a couple hours. Like SQL, Make is declarative. We describe the result, and let the program optimize the route by which we get there. Creating a simple project with make and python # I\u0026rsquo;m going to use a distilled example of a recent project I built using just a Makefile, some python, and a little SQL. By the end of this my hope is to show that simple tools can be efficient and reliable, and avoid the overhead of learning, installing, configuring, and inevitably debugging something more complex.2 Ultimately, I wanted to hand this project off in such a way that any of my teammates could maintain it if I was unavailable, so it had to be short, and stick to the tools I know they will always have installed.\nOur goal is to produce an Excel file for executive consumption that has a meaningful summary of some data pulled out of our analytics warehouse. Overall, it\u0026rsquo;ll look a little like this:\nbase queries --\u0026gt; summary CSVs --\u0026gt; (report.xlsx, diagrams for powerpoint) The \u0026ldquo;base\u0026rdquo; queries might look a lot like temporary tables or common table expressions (CTEs, or those blocks you see in WITH statements), but we\u0026rsquo;ve broken them into several, separate queries. We\u0026rsquo;re plopping those summarized results into some flat files so that we can examine the results with our favorite tools like pandas or awk. We\u0026rsquo;ll then take all those flat results and produce deliverables from them, like charts and an excel file.\nmyguy is always there for me, so that\u0026rsquo;s the name of our project, and its basic structure looks like this:\n. â”œâ”€â”€ config.yml â”œâ”€â”€ Makefile â”œâ”€â”€ myguy.py â”œâ”€â”€ README.md â””â”€â”€ sql â”œâ”€â”€ this_quarter_sales.sql â”œâ”€â”€ model_forecast.sql â””â”€â”€ customer_disposition.sql We will execute each file in the sql/ directory as a query we execute and then locally cache the results as CSVs. Later we\u0026rsquo;ll discuss how to handle the case where all our queries are handled remotely, and don\u0026rsquo;t create local files, such as running CREATE TABLE AS (ctas) queries prior to building the report, but for now we\u0026rsquo;ll keep it simple: query \u0026ndash;\u0026gt; csv on computer.\nOur goal is to make reproducing this report dead simple. I should only have to run this command to rebuild the report at any time:\nmake report.xlsx The cookbook that provides this rule is the Makefile.\n# Makefile report.xlsx: myguy.py python -m myguy build-report If this is your first time seeing make, there are a few terms to know:\nreport.xlsx - this is the target of the rule. It is the file produced by running make report.xlsx myguy.py - the prerequisite of report.xlsx. It has to exist in order to create the excel file. If this python file\u0026rsquo;s contents have changed recently, then that\u0026rsquo;s an indication that report.xlsx may also change. python -m myguy build-report - this is the recipe that Make runs when you issue the command make report.xlsx. I am invoking python with the -m, or \u0026ldquo;run module as main\u0026rdquo; flag in case we ever refactor our single .py file into a module, like myguy/__init__.py with its complementary \u0026ldquo;dunder main\u0026rdquo; myguy/__main__.py. Our main entry point will be in the python module. This will handle the program runtime for executing queries or doing some pandas hackery.\n# myguy.py from pathlib import Path from time import sleep import click import pandas as pd @click.group() def cli(): pass @cli.command() @click.option( \u0026#34;-o\u0026#34;, \u0026#34;--output\u0026#34;, help=\u0026#34;Write to file\u0026#34;, default=f\u0026#34;{BUILD_DIR}/report.xlsx\u0026#34;, show_default=True, ) def build_report(output: str): \u0026#34;\u0026#34;\u0026#34; Generate a new Excel workbook and save it locally. \u0026#34;\u0026#34;\u0026#34; # Suppose this produces, you know, useful data pd.DataFrame(dict(a=range(3), b=list(\u0026#34;abc\u0026#34;))).to_excel(output, index=False) sleep(2) print(f\u0026#34;Saved report to {output}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: cli() I think click is just great, and provides me with a lot of zen writing a command line interface (CLI) compared to argparse, although you could achieve everything I\u0026rsquo;m doing in this article equally well with argparse. In this script we create a cli group because we\u0026rsquo;ll eventually add more commands to it. The build_report function just replicates a process that takes a couple seconds before it outputs a file to build/report.xlsx.\nIt takes very little code to get a pleasant command line experience with nested commands. Here\u0026rsquo;s a quick example of using it after adding another command called query, which we\u0026rsquo;ll get to in a moment:\nHowever, if we try building our report right now with make report.xlsx, we get a FileNotFoundError: [Errno 2] No such file or directory: 'build/report.xlsx', and that\u0026rsquo;s because we need to make sure the build directory exists before running this command. We could handle that in the python with a few lines, but why not have our dependency management tool, make, do it for us?\n# Makefile build: mkdir -p build report.xlsx: myguy.py | build python -m myguy build-report Now our make report.xlsx works just fine, and we get a new directory build with our empty report in it. Normally we won\u0026rsquo;t need the |, but in this case it declares that the build rule should only be run once, even if we have other targets with build as a prerequisite.3 If we rerun make report.xlsx, it doesn\u0026rsquo;t try to create that directory again, because it already exists.\nWe do have one other problem though: if we re-run the python code, it will overwrite our report, even if nothing has changed. Instead, we should get a message saying make: 'report.xlsx' is up to date. This is happening because our target is report.xlsx instead of build/report.xlsx, so make looks in the current directory, sees that there\u0026rsquo;s no report.xlsx and therefore runs the recipe. I don\u0026rsquo;t want to write make build/report.xlsx, so what we\u0026rsquo;ll do is set up make to automatically look in our build and sql directories for files by setting the VPATH variable:\n# Makefile BUILDDIR := build SQLDIR := sql VPATH := $(SQLDIR):$(TARGETDIR) $(BUILDDIR): mkdir -p $(BUILDDIR) report.xlsx: myguy.py | $(BULDDIR) python -m myguy build-report By doing this, we can just issue make report.xlsx instead of make build/report.xlsx. Setting build to a variable (referenced via $(BUILDDIR)) allows us to change up the build directory on a whim, should we need to.\nNext, we need to structure the rules that handle our queries, so let\u0026rsquo;s add a generic method for doing exactly that, given the path to a sql file.\n# myguy.py, cont. from datetime import datetime # ...other content same as before... BUILD_DIR = \u0026#34;build\u0026#34; @cli.command() @click.argument(\u0026#34;path\u0026#34;) def query(path: str): \u0026#34;\u0026#34;\u0026#34; Issue the query located at `path` to the database, and write the results to a similarly-named CSV in the `build` directory. \\b Examples -------- This command will produce a new file foobar.csv in the `myguy.BUILD_DIR` directory: $ python -m myguy query sql/foobar.sql \u0026#34;\u0026#34;\u0026#34; # Hey look, more fake code for an article about querying data sleep(2) destination = Path(BUILD_DIR) / Path(path).with_suffix(\u0026#34;.csv\u0026#34;).name ( pd.DataFrame( dict(time_updated=[datetime.now()]) ) .to_csv(destination, index=False) ) print(f\u0026#34;Finished query for {path} and wrote results to {destination}\u0026#34;) Again, imagine that the \u0026ldquo;sleep\u0026rdquo; we\u0026rsquo;re doing here is some body of actual code that fetches results from the database. We also don\u0026rsquo;t need pandas for something as banal as touching a csv with today\u0026rsquo;s date, but it\u0026rsquo;s there to replicate the very common use case of pd.read_sql -\u0026gt; to_csv, which is nearly always the most efficient way to write a program that acquires a database connection, queries it, and writes the results to a csv for analytics-scale work in python like this.\nWe\u0026rsquo;ve also refactored out the destination build directory into a variable, so we can have the make script grab that automatically using the shell built-in:\n# Makefile BUILDDIR := $(shell python -c \u0026#39;import myguy; print(myguy.BUILD_DIR)\u0026#39;) This technique is also useful for having builds that depend on things like myguy.__version__, if it exists. With the python in place, we need to set up our recipes that run the queries. A naive first approach might look like this:\n# Makefile, cont. # ... same as above ... this_quarter_sales.csv: this_quarter_sales.sql python -m myguy query sql/this_quarter_sales.sql model_forecast.csv: model_forecast.sql python -m myguy query sql/model_forecast.sql customer_disposition.csv: customer_disposition.sql python -m myguy query sql/customer_disposition.sql That\u0026rsquo;s a lot of repetition, so our programmer instincts should kick in here and tell us that \u0026ldquo;there must be a better way!\u0026rdquo;4. Make can handle this using implicit rules. We specify a pattern like this, and Make will do all the hard work of connecting the files together:\n%.csv: %.sql myguy.py | $(BUILDDIR) @python -m myguy query $\u0026lt; This takes care of all three rules at once.\nThe % is a wildcard - it matches anything and our rules here say that \u0026ldquo;if we\u0026rsquo;re going to build a .csv, then it has a similarly-named .sql file as a prerequisite. The @ at the start of the recipe suppresses echoing of the command when Make runs it The $\u0026lt; is an automatic variable that stands for the first prerequisite. In our case, that will be the full path to the .sql file we are passing into the query function we previously wrote. Since myguy.py is the \u0026ldquo;source\u0026rdquo; of the running command, that\u0026rsquo;s also a prerequisite the | $(BUILDDIR), as before, says that we have an order-only prerequisite on the build directory, and our earlier rule will ensure mkdir build is run before trying to put output there Now we can issue the same command as our report to build the CSV from each of our queries:\n$ make this_quarter_sales.csv Finished query for sql/this_quarter_sales.sql and wrote results to build/this_quarter_sales.csv We still need to tie all these together so that I don\u0026rsquo;t have to run each command manually - we want to just run make report.xlsx and have it do all the prerequisite queries for us. To accomplish this, we\u0026rsquo;re going to use two more built-ins, wildcard and patsubst to build the prerequisite and target lists, respectively.\nSQLFILES := $(wildcard $(SQLDIR)/*.sql) TARGETS := $(patsubst $(SQLDIR)/%.sql,%.csv,$(SQLFILES)) If we were to echo the contents of these two variables, they would look like this:\n# contents of SQLFILES sql/customer_disposition.sql sql/model_forecast.sql sql/this_quarter_sales.sql # contents of TARGETS customer_disposition.csv model_forecast.csv this_quarter_sales.csv Since our report.xlsx depends on all three of the files in TARGETS, we bind them together in that rule:\n# Makefile # ... other content same as before ... report.xlsx: $(TARGETS) @python -m myguy build-report Note here that we took out the myguy.py and $(BUILDDIR) prerequisites from this rule, since those are coming from our implicit rule on the $(TARGETS). I\u0026rsquo;m also going to add a clean rule for trying things over from a fresh start:\nclean: @[ ! -d $(BUILDDIR) ] || rm -r $(BUILDDIR) Decomposing this into English:\n@ - don\u0026rsquo;t echo this command when it runs. Just run it. [ ! -d $(BUILDDIR) ] || - unless the BUILDDIR is missing, do the next command5 rm -r $(BUILDDIR) - remove the contents of the BUILDDIR recursively Here\u0026rsquo;s where our Makefile is now:\n# Makefile BUILDDIR := $(shell python -c \u0026#39;import myguy; print(myguy.BUILD_DIR)\u0026#39;) SQLDIR := sql VPATH := $(SQLDIR):$(TARGETDIR) SQLFILES := $(wildcard $(SQLDIR)/*.sql) TARGETS := $(patsubst $(SQLDIR)/%.sql,%.csv,$(SQLFILES)) report.xlsx: $(TARGETS) @python -m myguy build-report $(BUILDDIR): mkdir -p $(BUILDDIR) %.csv: %.sql myguy.py | $(BUILDDIR) @python -m myguy query $\u0026lt; clean: @[ ! -d $(BUILDDIR) ] || rm -r $(BUILDDIR) So now we can run clean and build the report:\nWe can do better though. Make has parallelism built in, and most of our computers have no trouble running things concurrently. By providing the -j (jobs) flag, we can tell it to do several things at once as long as they don\u0026rsquo;t depend on one another. Since our intermediate queries to CSV fit the bill, they can all run at the same time:\nIt\u0026rsquo;s also possible to enable parallelism by default with the MAKEFLAGS special variable.\n# Makefile MAKEFLAGS := --jobs=$(shell nproc) Adding dependencies between intermediate queries # Let\u0026rsquo;s suppose we add two more queries, and they need to run before our existing queries, because we did a little refactoring of our SQL.\nOur DAG Moreover, let\u0026rsquo;s assume that these two new tables are too large to cache locally in a flat file, and we have to issue a CREATE TABLE AS (ctas) statement to build them first. Not every database will permit you to run a CTAS on it, but imagine this is any process that writes data remotely instead of locally, such as spark writing a parquet file on S3, or submitting a POST request to an endpoint we don\u0026rsquo;t control.\nWe represent this in Make by first writing out the targets and prerequisites with different suffixes. For the remote tables, I\u0026rsquo;ll use the \u0026lsquo;.ctas\u0026rsquo; suffix. I also like to do this in a separate file, dag.mk, so I can hop to its buffer directly in my editor.\n# dag.mk sales_subset.ctas: customer_product.ctas: this_quarter_sales.csv model_forecast.csv: sales_subset.ctas customer_disposition.csv: customer_product.ctas This arrangement forces the completion of sales_subset.csv and customer_product.csv prior to the original three queries. Then in the main MakeFile, we include these contents above the rules that handle .csv files, along with a new rule for handling the remote tables. The %.ctas rule will create an empty target as soon as the query is done, signaling to make when it last completed successfully:\n# Makefile # ... other content the same ... include dag.mk %.csv: %.sql myguy.py | $(BUILDDIR) @python -m myguy query $\u0026lt; %.ctas: %.sql myguy.py | $(BUILDDIR) @python -m myguy ctas $\u0026lt; @touch $(BUILDDIR)/$@ Note that this includes a new command for myguy, so let\u0026rsquo;s add that too:\n# myguy.py @cli.command() @click.argument(\u0026#34;path\u0026#34;) def ctas(path: str): \u0026#34;\u0026#34;\u0026#34; Perform a CREATE TABLE AS statement from the SELECT statement in the given SQL file path. \u0026#34;\u0026#34;\u0026#34; sql_file = Path(path) query = f\u0026#34;CREATE TABLE {sql_file.stem} AS {sql_file.read_text()}\u0026#34; print(query) sleep(3) # imagine the query is running And one last thing - our $(TARGETS) assignment has no way of telling which sql files it should or shouldn\u0026rsquo;t tie to CSVs. The easiest way to make this distinction is to actually just remove $(TARGETS) altogether, and have the dag.mk declare what report.xlsx depends on.\n# dag.mk # ...other contents the same... report.xlsx: this_quarter_sales.csv \\ model_forecast.csv \\ customer_disposition.csv # Makefile report.xlsx: @python -m myguy build-report Make will take care of combining these two rules into a single one. If all the targets are going to have the same suffix, such as .ctas or .csv, then the trick with $(TARGETS) is handy, but adds more complexity than it\u0026rsquo;s worth when mixing target file types.\nAltogether, our dag now looks like this:\nTemplating the SQL with Jinja # To round out some of our feature parity with dbt, we need to add templating to our SQL. It\u0026rsquo;s best if I don\u0026rsquo;t have to think about how the template is injected, I just want a standard place to put stuff and have the python module take care of it. Let\u0026rsquo;s introduce a configuration file:\n# config.yml sales_max_date: \u0026#34;2022-03-01\u0026#34; sales_min_date: \u0026#34;2021-09-01\u0026#34; Presumably, a future analyst will have to come refresh this report for a different set of dates, and we want that configuration readily available to them. The templating engine we\u0026rsquo;ll use is the same on available on dbt, Jinja2. How we integrate it is by intercepting our python code that reads queries and applying the template from the config there. The jinja2 part is a little verbose, but flexible - anything that\u0026rsquo;s in our config will be available to the templated SQL:6\n# myguy.py import yaml import jinja2 # ... other content the same ... BUILD_DIR = \u0026#34;build\u0026#34; PROJECT_DIR = Path(__file__).parent def get_config(): with open(PROJECT_DIR / \u0026#34;config.yml\u0026#34;) as f: config = yaml.safe_load(f) return config # The `str | Path` union type hint is python 3.10 syntax, so watch out if you\u0026#39;re # on an older version! def read_sql_text(path: str | Path) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Read a SQL file contents and apply jinja templating from this project\u0026#39;s config.yml \u0026#34;\u0026#34;\u0026#34; config = get_config() jinja_loader = jinja2.FileSystemLoader(PROJECT_DIR) jinja_environment = jinja2.Environment(loader=jinja_loader) template = jinja_environment.get_template(str(path)) sql = template.render(**config) return sql And now we use this version of the sql in the ctas and query functions:\n# myguy.py # ...other content the same ... def query(path: str): sql = read_sql_text(path) destination = Path(BUILD_DIR) / Path(path).with_suffix(\u0026#34;.csv\u0026#34;).name # Assuming you have some way of acquiring a database connection with get_connection() as conn: pd.read_sql(sql, conn).to_csv(destination) def ctas(path: str): path = Path(path) sql = f\u0026#34;CREATE TABLE `{path.stem}` AS {read_sql_text(path)}\u0026#34; with get_connection() as conn: conn.execute(sql) Within the SQL itself, we can now reference any of the keys in the config.yml directly:\n# sales_subset.sql SELECT product_id , sales_revenue , sales_units FROM fact_sales WHERE sales_date BETWEEN date(\u0026#39;{{ sales_min_date }}\u0026#39;) AND date(\u0026#39;{{ sales_max_date }}\u0026#39;) I\u0026rsquo;m not here to cover everything you can do with jinja and yaml, since those are already pretty well covered. If you haven\u0026rsquo;t used it before, it\u0026rsquo;s worth looking into. It\u0026rsquo;s very powerful. The looping and conditional constructs can make what would normally be pretty tough with just raw SQL easy. When we issue a make recent_sales.ctas command, it looks like this:\n$ make sales_subset.ctas mkdir -p build CREATE TABLE `sales_subset` AS SELECT product_id , sales_revenue , sales_units FROM fact_sales WHERE sales_date BETWEEN date(\u0026#39;2021-09-01\u0026#39;) AND date(\u0026#39;2022-03-01\u0026#39;) Conclusion # There we have it, a simple little dag system for coordinating our project\u0026rsquo;s deliverables. For a working example of how to implement this project structure, check out the companion repo, which builds a simple analysis on the chinook dataset. This example also includes code that produces .png files for including into presentations and migrates the mypy.py into a fully-fledged, pip-installable module.\nExcept Windows. You\u0026rsquo;ll need to get it via mingw/cygwin or via the Windows subsystem for Linux.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI fully acknowledge the irony here that make is, in fact, a very foreign tool to many data scientists.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThese are called order-only prerequisites\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;m taking this phrase from Raymond Hettinger, who gives fantastic talks on writing idomatic python. I recommend his beyond PEP8 talk to all levels of developers.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe do \u0026ldquo;unless\u0026rdquo; instead of the more familiar \u0026ldquo;if\u0026rdquo; statement, because if we did this: [ -d $(BUILDDIR) ] \u0026amp;\u0026amp; rm -r $(BUILDDIR) the test command [ exits with status 1 when the build directory doesn\u0026rsquo;t exist, and hence the whole pipe exits status 1. Make treats that as a failed recipe, which isn\u0026rsquo;t what we intend. We want it to look like a success both in the case of removing the directory should it exist, and doing nothing if it doesn\u0026rsquo;t.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou should be extremely careful about templating like this. This article also uses f-strings for injecting arbitrary code into SQL, which is unacceptable if any of your system is public facing. All of these examples are working under the assumption that we\u0026rsquo;re in a locked-down internal data warehouse, and someone who has access to the system in any way is allowed to issue an arbitrary query.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"6 March 2022","permalink":"/posts/000_make_dag/","section":"Posts","summary":"In data science and engineering we tend to think in \u0026ldquo;DAGs\u0026rdquo; (directed acyclic graphs), which just means \u0026ldquo;to make this report, we first have to build this other thing, and to build that thing, we have to run these two queries, and so on.","title":"Building DAGs with Make, SQL, and python"},{"content":"Why create a new tech blog when so many great ones already exist? In short, because data scientists often get the advice that they should \u0026ldquo;improve their software engineering skills,\u0026rdquo; but the advice they are given on how to do so is usually terrible1. Data science, from a programming perspective, is in a weird place. We aren\u0026rsquo;t quite front end, but we create maps, charts, and plots to display in the browser all the time. We aren\u0026rsquo;t quite back end, yet we write code that targets the file system, databases, and external APIs. And we aren\u0026rsquo;t quite consultants, but still have to be the expert, data-driven decision maker. Data science is not a slow moving field, either, which leaves little space for pausing to reflect on how to write better code.\nI primarily work in a world that uses python, SQL, the standard GNU toolkit (bash, make, vim), and the usual host of data-sciency things that come with all that, like Jupyter and plotly. My posts will be largely targeted at an audience who intends to become more efficient with these tools, or to introduce those who want to become better coders, but aren\u0026rsquo;t sure where to start, to some of the tools that can improve your data science workflow. For about five years now, I\u0026rsquo;ve mentored Data Science team members from writing their first line of code to maintaining 100k \u0026ldquo;source lines of code\u0026rdquo; (SLOC) code bases2. I\u0026rsquo;ve received (and given) many questions prefaced with \u0026ldquo;this may be a stupid question, but\u0026hellip;\u0026rdquo;, and thought that it was about time I wrote down the answers for others to learn from.\nAs of writing, it\u0026rsquo;s February of 2022 and I still see Reddit comments telling new python programmers that requirements.txt is an acceptable form of project dependency management.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSeasoned programming veterans out there might scoff at how low 100k sounds, but for those accustomed to working mainly in Jupyter notebooks, this can be a daunting amount of code.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"26 February 2022","permalink":"/posts/my-first-post/","section":"Posts","summary":"Why create a new tech blog when so many great ones already exist?","title":"Objective"}]